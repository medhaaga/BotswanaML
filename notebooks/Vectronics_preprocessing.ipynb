{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa2c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11be33",
   "metadata": {},
   "source": [
    "# Vectronics Preprocesisng\n",
    "\n",
    "**Author:** Medha Agarwal | \n",
    "**Last Modified:** December 12, 2025\n",
    "\n",
    "This notebook documents the preprocessing pipeline used to generate summary-level Vectronics data for training models that are subsequently applied to RVC data. The raw Vectronics data are collected at a high temporal resolution (16 Hz). To align these data with the coarser temporal granularity of the RVC dataset, we aggregate the raw signals into 30-second windows and compute summary statistics for each window.\n",
    "\n",
    "Using the raw Vectronics data together with behavior annotations, we construct two primary data products:\n",
    "\n",
    "1. **Labeled Vectronics summary data**\n",
    "2. **Unlabeled Vectronics summary data**\n",
    "\n",
    "### Required Files\n",
    "\n",
    "The notebook expects the following input files:\n",
    "\n",
    "1. Video annotations: `data/2025_10_31_awd_video_annotations.csv`  \n",
    "2. Audio annotations: `data/2025_10_31_awd_audio_annotations.csv`  \n",
    "3. Metadata: `data/metadata.csv`  \n",
    "4. Vectronics acceleration files, organized by half-day as specified in the `file path` column of the metadata file\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Labeled Vectronics Summary Data\n",
    "\n",
    "The labeled summary dataset is created through the following sequence of steps:\n",
    "\n",
    "1.1. **Annotation matching** : Match the high-frequency Vectronics data with the available behavior annotations.\n",
    "\n",
    "1.2. **Window extraction** : Segment the matched portions of the Vectronics data into non-overlapping 30-second windows.\n",
    "\n",
    "1.3. **Feature computation**: Compute nine summary statistics for each 30-second window.\n",
    "\n",
    "Finally, the resulting summarized Vectronics dataset is saved for downstream modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Unlabeled Vectronics Summary Data\n",
    "\n",
    "The unlabeled summary dataset is generated as follows:\n",
    "\n",
    "2.1. **Data loading**: Load raw Vectronics data for a given (animal ID, date) pair.\n",
    "\n",
    "2.2. **Windowing**: Partition the data into consecutive 30-second windows.\n",
    "\n",
    "2.3. **Feature computation**: Compute the same nine summary statistics for each window.\n",
    "\n",
    "2.4. **Iteration over metadata**: Repeat steps 1–3 for all unique (animal ID, date) pairs listed in the metadata.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementation Notes\n",
    "All preprocessing steps described above are implemented in  \n",
    "`scripts/run_Vectronics_preprocessing.py`.  \n",
    "This notebook provides a step-by-step explanation of the pipeline and illustrates each stage of the preprocessing process in detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d7f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2382885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.io as io\n",
    "from src.utils.Vectronics_preprocessing import (create_max_windows,\n",
    "                                                create_summary_data,\n",
    "                                                load_annotations,\n",
    "                                                create_windowed_features)\n",
    "import config as config\n",
    "from src.utils.data_prep import create_matched_data, create_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2314df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.VECTRONICS_PREPROCESSING_YAML) as f:\n",
    "    Vectronics_preprocessing_config = yaml.safe_load(f)\n",
    "\n",
    "window_duration = Vectronics_preprocessing_config['window_duration']\n",
    "window_length = int(window_duration*config.SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1f0f2",
   "metadata": {},
   "source": [
    "## 1. Labeled Vectronics Summary Data\n",
    "\n",
    "We begin by loading the behavior annotations using the function `load_annotations`. This function merges video-based annotations obtained from `io.get_video_labels_path()` with audio-based annotations obtained from `io.get_audio_labels_path()` into a single consolidated table.\n",
    "\n",
    "The resulting `all_annotations` CSV is expected to contain the following columns:\n",
    "\n",
    "- `id`  \n",
    "- `Behavior`  \n",
    "- `Timestamp_start`  \n",
    "- `Timestamp_end`  \n",
    "- `Source`  \n",
    "- `Confidence (H-M-L)`  \n",
    "- `Eating intensity`  \n",
    "- `duration`\n",
    "\n",
    "In addition to the annotations, a metadata CSV is created to organize and iterate over the Vectronics files saved as half days. The expected columns in the metadata file are:\n",
    "\n",
    "- `file path`  \n",
    "- `individual ID`  \n",
    "- `year`  \n",
    "- `UTC Date [yyyy-mm-dd]`  \n",
    "- `am/pm`  \n",
    "- `half day [yyyy-mm-dd_am/pm]`  \n",
    "- `avg temperature [C]`\n",
    "\n",
    "This metadata enables identification of unique animal–date combinations and provides contextual information (e.g., recording period and temperature) for each Vectronics file.\n",
    "\n",
    "> **Note:** Generating this metadata is computationally expensive and takes approximately **50 minutes** to run. This step should be executed only once to create the metadata corresponding to your local data paths. The resulting file is saved in the `data/` directory and can be reused in subsequent runs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed693544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual jessie has 506 halfdays.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [05:33<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual green has 900 halfdays.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [10:57<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual palus has 744 halfdays.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 744/744 [09:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual ash has 792 halfdays.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [09:51<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual fossey has 886 halfdays.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 886/886 [11:35<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# load annotations and metadata\n",
    "\n",
    "all_annotations = load_annotations()\n",
    "if not os.path.exists(io.get_vectronics_metadata_path()):\n",
    "    create_metadata(config.AWD_VECTRONICS_PATHS, io.get_vectronics_metadata_path())\n",
    "metadata = pd.read_csv(io.get_vectronics_metadata_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86784c58",
   "metadata": {},
   "source": [
    "#### Step 1.1: Annotations Matching\n",
    "\n",
    "To ensure that each 30-second window is associated with a well-defined behavioral label, we enforce a majority-behavior criterion within each window. Specifically, the behavior assigned to a window must be the dominant annotated behavior over that interval. We apply the following procedure:\n",
    "\n",
    "- For each contiguous annotation segment with duration greater than 15 seconds, we extract the corresponding high-frequency Vectronics signal.\n",
    "- These matched signal segments are symmetrically padded on the left and right until a total duration of 30 seconds is reached. \n",
    "- Annotation segments with duration shorter than 15 seconds are discarded later and are not used to create labeled Vectronics summary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2657a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual jessie has 506 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for jessie: 100%|██████████| 506/506 [00:53<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual green has 900 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for green: 100%|██████████| 900/900 [00:37<00:00, 24.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual palus has 744 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for palus: 100%|██████████| 744/744 [00:22<00:00, 32.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual ash has 792 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for ash: 100%|██████████| 792/792 [00:51<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual fossey has 448 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for fossey: 100%|██████████| 448/448 [00:39<00:00, 11.22it/s]\n"
     ]
    }
   ],
   "source": [
    "min_window_for_padding = 15.0\n",
    "_, acc_data, _, _ = create_matched_data(filtered_metadata=metadata, \n",
    "                                                            annotations=all_annotations, \n",
    "                                                            verbose=True, \n",
    "                                                            min_window_for_padding=min_window_for_padding,\n",
    "                                                            min_matched_duration=window_duration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aff1f6",
   "metadata": {},
   "source": [
    "#### Step 1.2: Window Extraction\n",
    "\n",
    "From the matched and padded signal segments obtained in Step 1.1, we extract fixed-length 30-second windows for downstream processing.\n",
    "\n",
    "- For all matched signal chunks with total duration greater than 30 seconds, we segment the data into 30-second windows.\n",
    "- After window extraction, any remaining matched chunks with duration shorter than 30 seconds are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa10fd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating windows of durations 30.0...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating windows of durations {window_duration}...\")\n",
    "acc_data_split = create_max_windows(acc_data=acc_data, window_duration=window_duration, sampling_rate=config.SAMPLING_RATE)\n",
    "acc_data_split = acc_data_split[acc_data_split.duration >= window_duration]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c1537",
   "metadata": {},
   "source": [
    "#### Step 1.3: Feature Computation\n",
    "\n",
    "With the 30-second windows defined in Step 1.2, we compute a set of summary statistics for each window. \n",
    "\n",
    "- Each 30-second window is summarized using **nine predefined statistics** (mean peak-to-peak, max peak-to-peak, amean acceleration for X, Y, and Z axis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e03d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating summary statistics...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating summary statistics...\")\n",
    "df_preprocessed = create_summary_data(acc_data_split, sampling_rate=config.SAMPLING_RATE)\n",
    "df_preprocessed = df_preprocessed.drop(columns=['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee87c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed data to /home/medhaaga/BotswanaML/data/Vectronics_preprocessed_duration30.0.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving preprocessed data to {io.get_Vectronics_preprocessed_path(window_duration)}\")\n",
    "df_preprocessed.to_csv(io.get_Vectronics_preprocessed_path(window_duration), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007297b",
   "metadata": {},
   "source": [
    "## 2. Unlabeled Vectronics Summary Data\n",
    "\n",
    "The unlabeled Vectronics data are processed similarly to the labeled data but without associating behavioral annotations. Due to the large volume of raw Vectronics recordings, generating the full summary dataset can take approximately **50 minutes** to complete. \n",
    "\n",
    "**Recommendation:**  \n",
    "For full-scale preprocessing, it is advisable to run the script  \n",
    "`scripts/run_Vectronics_preprocessing.py` from the terminal.\n",
    "\n",
    "In this notebook, we demonstrate the preprocessing on a **subset of 10 random (animal ID, date) pairs** as a sanity check. This allows verification of the pipeline without incurring the full runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6683d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = metadata.groupby([\"individual ID\", \"UTC Date [yyyy-mm-dd]\"])\n",
    "group_keys = list(grouped.groups.keys())   # list of (individual, date) tuples\n",
    "np.random.shuffle(group_keys)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fe4fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1708 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1708 [00:16<52:43,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "i = 0\n",
    "\n",
    "for individual, date in tqdm(group_keys, total=len(group_keys)):\n",
    "    group = grouped.get_group((individual, date))\n",
    "\n",
    "    # load all half-day files for this animal/day\n",
    "    dfs = []\n",
    "    for _, row in group.iterrows():\n",
    "        df_half = pd.read_csv(row['file path'])\n",
    "        df_half['Timestamp'] = pd.to_datetime(df_half['Timestamp'], utc=True, format='%Y-%m-%d %H:%M:%S.%f')\n",
    "        dfs.append(df_half)\n",
    "\n",
    "    full_day_data = pd.concat(dfs, ignore_index=True).sort_values(\"Timestamp\")\n",
    "\n",
    "    if len(full_day_data) < window_length:\n",
    "        warnings.warn(f'{individual}-{date} has fewer samples than the window length. Skipped.')\n",
    "        continue\n",
    "\n",
    "    features = create_windowed_features(full_day_data, sampling_frequency=config.SAMPLING_RATE, \n",
    "                                        window_duration=window_duration, window_length=window_length)\n",
    "    features['animal_id'] = individual\n",
    "    features['UTC date [yyyy-mm-dd]'] = date\n",
    "    results.append(features) \n",
    "\n",
    "    i+=1\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "df = pd.concat(results, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving preprocessed data to {io.get_Vectronics_full_summary_path()}\")\n",
    "df.to_csv(io.get_Vectronics_full_summary_path(), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildlife",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
