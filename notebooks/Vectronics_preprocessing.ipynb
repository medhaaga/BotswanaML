{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aa2c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11be33",
   "metadata": {},
   "source": [
    "# Vectronics Preprocessing\n",
    "\n",
    "**Author:** Medha Agarwal | \n",
    "**Last Modified:** December 12, 2025\n",
    "\n",
    "This notebook documents the preprocessing pipeline used to generate summary-level Vectronics data for training models that are subsequently applied to RVC data. The raw Vectronics data are collected at a high temporal resolution (16 Hz). To align these data with the coarser temporal granularity of the RVC dataset, we aggregate the raw signals into 30-second windows and compute summary statistics for each window.\n",
    "\n",
    "Using the raw Vectronics data together with behavior annotations, we construct two primary data products:\n",
    "\n",
    "1. **Labeled Vectronics summary data**\n",
    "2. **Unlabeled Vectronics summary data**\n",
    "\n",
    "### Required Files\n",
    "\n",
    "The notebook expects the following input files:\n",
    "\n",
    "1. Video annotations: `data/2025_10_31_awd_video_annotations.csv`  \n",
    "2. Audio annotations: `data/2025_10_31_awd_audio_annotations.csv`  \n",
    "3. Raw Vectronics data: `config.AWD_VECTRONICS_PATHS` (dictionary of directories for each individual. The key is the individual name ('jessie', 'palus', etc) and value is the path to the directories containing yearly CSV files of raw data). Please adjust the paths in this dictionary in `/config.paths.py`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Labeled Vectronics Summary Data\n",
    "\n",
    "The labeled summary dataset is created through the following sequence of steps:\n",
    "\n",
    "1.1 **Creating halfday segments & metadata**: Split the raw vectronics data into halfday segments from raw acceleration files stored in separate folders for each individual. Store the features of each halfday file (filepath individual, halfday, etc) in a metadats file for easy indexing.\n",
    "\n",
    "1.2. **Annotation matching** : Match the high-frequency Vectronics data with the available behavior annotations.\n",
    "\n",
    "1.3. **Window extraction** : Segment the matched portions of the Vectronics data into non-overlapping 30-second windows.\n",
    "\n",
    "1.4. **Feature computation**: Compute nine summary statistics for each 30-second window.\n",
    "\n",
    "Finally, the resulting summarized Vectronics dataset is saved for downstream modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Unlabeled Vectronics Summary Data\n",
    "\n",
    "The unlabeled summary dataset is generated as follows:\n",
    "\n",
    "2.1. **Data loading**: Load raw Vectronics data for a given (animal ID, date) pair.\n",
    "\n",
    "2.2. **Windowing**: Partition the data into consecutive 30-second windows.\n",
    "\n",
    "2.3. **Feature computation**: Compute the same nine summary statistics for each window.\n",
    "\n",
    "2.4. **Iteration over metadata**: Repeat steps 1–3 for all unique (animal ID, date) pairs listed in the metadata.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementation Notes\n",
    "All preprocessing steps described above are implemented in  \n",
    "`scripts/run_Vectronics_preprocessing.py`.  \n",
    "This notebook provides a step-by-step explanation of the pipeline and illustrates each stage of the preprocessing process in detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46d7f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2382885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.io as io\n",
    "from src.utils.vectronics_preprocessing import (create_max_windows,\n",
    "                                                create_summary_data,\n",
    "                                                load_annotations,\n",
    "                                                create_windowed_features)\n",
    "from src.utils.vectronics_data_prep import (create_vectronics_halfday_segments)\n",
    "import config as config\n",
    "from src.utils.vectronics_data_prep import create_matched_data, create_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2314df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.VECTRONICS_PREPROCESSING_YAML) as f:\n",
    "    Vectronics_preprocessing_config = yaml.safe_load(f)\n",
    "\n",
    "window_duration = Vectronics_preprocessing_config['window_duration']\n",
    "window_length = int(window_duration*config.SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74fba1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "\n",
    "    # ---------------- Preprocessing ----------------\n",
    "    min_window_for_padding: Optional[float] = None # minimum duration of signal beyond which pad the signal, None means no padding\n",
    "    seed: int = 0\n",
    "\n",
    "    # ---------------- Data ----------------\n",
    "    source_data_path: Optional[str] = None\n",
    "    \n",
    "args = TrainConfig()\n",
    "\n",
    "np.random.seed(seed=args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1f0f2",
   "metadata": {},
   "source": [
    "## 1. Labeled Vectronics Summary Data\n",
    "\n",
    "We begin by creating half day segments from the raw Vectronics data which is expected to be stored in the following format:  \n",
    "\n",
    "- root data directory\n",
    "  - individual 1\n",
    "    - 2022.csv\n",
    "    - 2023.csv\n",
    "    - 2024.csv\n",
    "  - individual 2\n",
    "    - 2022.csv\n",
    "    - 2023.csv\n",
    "  - individual 3\n",
    "    - 2023.csv\n",
    "    - 2024.csv\n",
    "\n",
    "**Note**: Individual name cannot contain an ``undercsore`` ($\\_$).\n",
    "\n",
    "Each acceleration data CSV is expected to have columns \n",
    "| Column Name                     | Data Type | Description                                      |\n",
    "|---------------------------------|-----------|--------------------------------------------------|\n",
    "| UTC Date[mm/dd]                 | string    | date in format mm/dd                             |\n",
    "| UTC DateTime                    | string    | time in format HH:MM:SS                          |\n",
    "| Milliseconds                    | int       | milliseconds (three digits)                      |\n",
    "| Acc X [g]                       | float     | acceleration reading along X axis                |\n",
    "| Acc Y [g]                       | float     | acceleration reading along Y axis                |\n",
    "| Acc Z [g]                       | float     | acceleration reading along Z axis                |\n",
    "| Temperature [Celsius]           | float     | temperature reading in celsius                   |\n",
    "\n",
    "\n",
    "Based on this a `combined_acc` directory is created inside each individual's directory which stores the data split into half days. To easily index the halfday segements, a metadata CSV is created to organize and iterate over the Vectronics files saved as half days. The expected columns in the metadata file are:\n",
    "\n",
    "- `file path`  \n",
    "- `individual ID`  \n",
    "- `year`  \n",
    "- `UTC Date [yyyy-mm-dd]`  \n",
    "- `am/pm`  \n",
    "- `half day [yyyy-mm-dd_am/pm]`  \n",
    "- `avg temperature [C]`\n",
    "\n",
    "\n",
    "Next, we load the behavior annotations using the function `load_annotations`. This function merges video-based annotations obtained from `io.get_video_labels_path()` with audio-based annotations obtained from `io.get_audio_labels_path()` into a single consolidated table.\n",
    "The resulting `all_annotations` CSV is expected to contain the following columns:\n",
    "\n",
    "- `id`  \n",
    "- `Behavior`  \n",
    "- `Timestamp_start`  \n",
    "- `Timestamp_end`  \n",
    "- `Source`  \n",
    "- `Confidence (H-M-L)`  \n",
    "- `Eating intensity`  \n",
    "- `duration`\n",
    "\n",
    "\n",
    "\n",
    "This metadata enables identification of unique animal–date combinations and provides contextual information (e.g., recording period and temperature) for each Vectronics file.\n",
    "\n",
    "> **Note:** Generating this metadata is computationally expensive and takes approximately **50 minutes** to run. This step should be executed only once to create the metadata corresponding to your local data paths. The resulting file is saved in the `data/` directory and can be reused in subsequent runs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c8c2d",
   "metadata": {},
   "source": [
    "#### Step 1.1: Creating half-day segments and metadata\n",
    "\n",
    "Generating half-day segments and associated metadata from approximately two years of data across five dogs takes **~2.5 hours** to complete. Creating the metadata takes **~20 minutes**. For full-scale processing, it is recommended to run the script `src/utils/vectronics_data_prep.py` in the background (e.g., using `tmux`).\n",
    "\n",
    "In this tutorial, we generate halfdays from **only one chunk (17.36 hours long)** per yearly CSV per individual to validate that the pipeline is functioning correctly. This is achieved by setting `max_chunks=1` in the relevant data-processing functions. \n",
    "\n",
    "Using the default value `max_chunks=None` processes **all chunks**, thereby generating half-day files for the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing individual:         jessie\n",
      "Files for this individual :    ['2023.csv', '2022.csv']\n",
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2022_44934_Samurai_Jessie/2023.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2023-01-01_AM' '2023-01-01_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:39<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2022_44934_Samurai_Jessie/2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2022-05-09_PM' '2022-05-09_AM' '2022-05-10_AM' '2022-05-10_PM'\n",
      " '2022-05-11_AM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:20<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing individual:         green\n",
      "Files for this individual :    ['2021.csv', '2023.csv', '2022.csv']\n",
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44915_Samurai_Green/2021.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2021-03-24_PM' '2021-03-24_AM' '2021-03-25_AM' '2021-03-25_PM'\n",
      " '2021-03-26_AM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:20<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44915_Samurai_Green/2023.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2023-01-26_AM' '2023-02-09_PM'], chunk duration: 0.26 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:00<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44915_Samurai_Green/2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2022-01-01_AM' '2022-01-01_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:15<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing individual:         palus\n",
      "Files for this individual :    ['2021.csv', '2022.csv']\n",
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44910_Aqua_Palus/2021.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2021-03-24_PM' '2021-03-24_AM' '2021-03-25_AM' '2021-03-25_PM'\n",
      " '2021-03-26_AM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:16<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44910_Aqua_Palus/2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2022-01-01_AM' '2022-01-01_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:15<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing individual:         ash\n",
      "Files for this individual :    ['2021.csv', '2022.csv']\n",
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44904_Ninja_Ash/2021.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2021-03-24_PM' '2021-03-24_AM' '2021-03-25_AM' '2021-03-25_PM'\n",
      " '2021-03-26_AM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:15<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44904_Ninja_Ash/2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2022-01-01_AM' '2022-01-01_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:16<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing individual:         fossey\n",
      "Files for this individual :    ['2023.csv', '2022.csv']\n",
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2022_44907_Aqua_Fossey/2023.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2023-01-01_AM' '2023-01-01_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:16<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2022_44907_Aqua_Fossey/2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2022-06-12_AM' '2022-06-12_PM' '2022-06-13_AM' '2022-06-13_PM'\n",
      " '2022-06-14_AM' '2022-06-14_PM' '2022-06-23_AM' '2022-07-03_AM'\n",
      " '2022-07-03_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:16<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create half-day segments of vectronics data by reading it in chunks\n",
    "create_vectronics_halfday_segments(config.VECTRONICS_PATHS, max_chunks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649d617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata is already saved in /home/medhaaga/BotswanaML/data/vectronics_metadata.csv. Creation skipped.\n"
     ]
    }
   ],
   "source": [
    "# create a metadata of the vectronics data\n",
    "if os.path.exists(io.get_vectronics_metadata_path()):\n",
    "    print(f\"Metadata is already saved in {io.get_vectronics_metadata_path()}. Creation skipped.\")\n",
    "else:\n",
    "    create_metadata(config.VECTRONICS_PATHS, io.get_vectronics_metadata_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed693544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations and metadata\n",
    "\n",
    "all_annotations = load_annotations()\n",
    "metadata = pd.read_csv(io.get_vectronics_metadata_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86784c58",
   "metadata": {},
   "source": [
    "#### Step 1.2: Annotations Matching\n",
    "\n",
    "To ensure that each 30-second window is associated with a well-defined behavioral label, we enforce a majority-behavior criterion within each window. Specifically, the behavior assigned to a window must be the dominant annotated behavior over that interval. We apply the following procedure:\n",
    "\n",
    "- For each contiguous annotation segment with duration greater than 15 seconds, we extract the corresponding high-frequency Vectronics signal.\n",
    "- These matched signal segments are symmetrically padded on the left and right until a total duration of 30 seconds is reached. \n",
    "- Annotation segments with duration shorter than 15 seconds are discarded later and are not used to create labeled Vectronics summary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual jessie has 506 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for jessie: 100%|██████████| 506/506 [00:53<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual green has 900 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for green: 100%|██████████| 900/900 [00:37<00:00, 24.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual palus has 744 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for palus: 100%|██████████| 744/744 [00:22<00:00, 32.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual ash has 792 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for ash: 100%|██████████| 792/792 [00:51<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual fossey has 448 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for fossey: 100%|██████████| 448/448 [00:39<00:00, 11.22it/s]\n"
     ]
    }
   ],
   "source": [
    "_, acc_data, _, _ = create_matched_data(filtered_metadata=metadata, \n",
    "                                                            annotations=all_annotations, \n",
    "                                                            verbose=True, \n",
    "                                                            min_window_for_padding=args.min_window_for_padding,\n",
    "                                                            min_matched_duration=window_duration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aff1f6",
   "metadata": {},
   "source": [
    "#### Step 1.3: Window Extraction\n",
    "\n",
    "From the matched and padded signal segments obtained in Step 1.1, we extract fixed-length 30-second windows for downstream processing.\n",
    "\n",
    "- For all matched signal chunks with total duration greater than 30 seconds, we segment the data into 30-second windows.\n",
    "- After window extraction, any remaining matched chunks with duration shorter than 30 seconds are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa10fd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating windows of durations 30.0...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating windows of durations {window_duration}...\")\n",
    "acc_data_split = create_max_windows(acc_data=acc_data, window_duration=window_duration, sampling_rate=config.SAMPLING_RATE)\n",
    "acc_data_split = acc_data_split[acc_data_split.duration >= window_duration]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c1537",
   "metadata": {},
   "source": [
    "#### Step 1.4: Feature Computation\n",
    "\n",
    "With the 30-second windows defined in Step 1.2, we compute a set of summary statistics for each window. \n",
    "\n",
    "- Each 30-second window is summarized using **nine predefined statistics** (mean peak-to-peak, max peak-to-peak, amean acceleration for X, Y, and Z axis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e03d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating summary statistics...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating summary statistics...\")\n",
    "df_preprocessed = create_summary_data(acc_data_split, sampling_rate=config.SAMPLING_RATE)\n",
    "df_preprocessed = df_preprocessed.drop(columns=['duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37cb6f",
   "metadata": {},
   "source": [
    "The preprocessed Vectronics data is saved in `args.source_data_path` if it is not `None`. Otherwise, the Vectronics data is stored in the path returned by\n",
    "`io.get_Vectronics_preprocessed_path(args.source_padding_duration)`. When `args.source_padding_duration` is `None` (i.e., no padding is applied), this function resolves to the default path:\n",
    "\n",
    "```\n",
    "/data/Vectronics_preprocessed.csv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed data to /home/medhaaga/BotswanaML/data/Vectronics_preprocessed_duration30.0.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving preprocessed data to {io.get_Vectronics_preprocessed_path(window_duration)}\")\n",
    "if args.source_data_path is not None:\n",
    "    df_preprocessed.to_csv(args.source_data_path, index=False)\n",
    "else:\n",
    "    df_preprocessed.to_csv(io.get_Vectronics_preprocessed_path(window_duration), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007297b",
   "metadata": {},
   "source": [
    "## 2. Unlabeled Vectronics Summary Data\n",
    "\n",
    "The unlabeled Vectronics data are processed similarly to the labeled data but without associating behavioral annotations. Due to the large volume of raw Vectronics recordings, generating the full summary dataset can take approximately **50 minutes** to complete. \n",
    "\n",
    "**Recommendation:**  \n",
    "For full-scale preprocessing, it is advisable to run the script  \n",
    "`scripts/run_Vectronics_preprocessing.py` from the terminal.\n",
    "\n",
    "In this notebook, we demonstrate the preprocessing on a **subset of 10 random (animal ID, date) pairs** as a sanity check. This allows verification of the pipeline without incurring the full runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6683d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = metadata.groupby([\"individual ID\", \"UTC Date [yyyy-mm-dd]\"])\n",
    "group_keys = list(grouped.groups.keys())   # list of (individual, date) tuples\n",
    "np.random.shuffle(group_keys)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fe4fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1708 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1708 [00:16<52:43,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "i = 0\n",
    "\n",
    "for individual, date in tqdm(group_keys, total=len(group_keys)):\n",
    "    group = grouped.get_group((individual, date))\n",
    "\n",
    "    # load all half-day files for this animal/day\n",
    "    dfs = []\n",
    "    for _, row in group.iterrows():\n",
    "        df_half = pd.read_csv(row['file path'])\n",
    "        df_half['Timestamp'] = pd.to_datetime(df_half['Timestamp'], utc=True, format='%Y-%m-%d %H:%M:%S.%f')\n",
    "        dfs.append(df_half)\n",
    "\n",
    "    full_day_data = pd.concat(dfs, ignore_index=True).sort_values(\"Timestamp\")\n",
    "\n",
    "    if len(full_day_data) < window_length:\n",
    "        warnings.warn(f'{individual}-{date} has fewer samples than the window length. Skipped.')\n",
    "        continue\n",
    "\n",
    "    features = create_windowed_features(full_day_data, sampling_frequency=config.SAMPLING_RATE, \n",
    "                                        window_duration=window_duration, window_length=window_length)\n",
    "    features['animal_id'] = individual\n",
    "    features['UTC date [yyyy-mm-dd]'] = date\n",
    "    results.append(features) \n",
    "\n",
    "    i+=1\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "df = pd.concat(results, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving preprocessed data to {io.get_Vectronics_full_summary_path()}\")\n",
    "df.to_csv(io.get_Vectronics_full_summary_path(), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildlife",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
