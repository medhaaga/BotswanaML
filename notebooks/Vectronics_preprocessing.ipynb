{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa2c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11be33",
   "metadata": {},
   "source": [
    "# Vectronics Preprocessing\n",
    "\n",
    "**Author:** Medha Agarwal | \n",
    "**Last Modified:** December 12, 2025\n",
    "\n",
    "This notebook documents the preprocessing pipeline used to generate summary-level Vectronics data for training models that are subsequently applied to RVC data. The raw Vectronics data are collected at a high temporal resolution (16 Hz). To align these data with the coarser temporal granularity of the RVC dataset, we aggregate the raw signals into 30-second windows and compute summary statistics for each window.\n",
    "\n",
    "Using the raw Vectronics data together with behavior annotations, we construct two primary data products:\n",
    "\n",
    "1. **Labeled Vectronics summary data**\n",
    "2. **Unlabeled Vectronics summary data**\n",
    "\n",
    "### Required Files\n",
    "\n",
    "The notebook expects the following input files:\n",
    "\n",
    "1. Video annotations: `data/2025_10_31_awd_video_annotations.csv`  \n",
    "2. Audio annotations: `data/2025_10_31_awd_audio_annotations.csv`  \n",
    "3. Raw Vectronics data: `config.AWD_VECTRONICS_PATHS` (dictionary of directories for each individual. The key is the individual name ('jessie', 'palus', etc) and value is the path to the directories containing yearly CSV files of raw data). Please adjust the paths in this dictionary in `/config.paths.py`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Labeled Vectronics Summary Data\n",
    "\n",
    "The labeled summary dataset is created through the following sequence of steps:\n",
    "\n",
    "1.1 **Creating halfday segments & metadata**: Split the raw vectronics data into halfday segments from raw acceleration files stored in separate folders for each individual. Store the features of each halfday file (filepath individual, halfday, etc) in a metadats file for easy indexing.\n",
    "\n",
    "1.2. **Annotation matching** : Match the high-frequency Vectronics data with the available behavior annotations.\n",
    "\n",
    "1.3. **Window extraction** : Segment the matched portions of the Vectronics data into non-overlapping 30-second windows.\n",
    "\n",
    "1.4. **Feature computation**: Compute nine summary statistics for each 30-second window.\n",
    "\n",
    "Finally, the resulting summarized Vectronics dataset is saved for downstream modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Unlabeled Vectronics Summary Data\n",
    "\n",
    "The unlabeled summary dataset is generated as follows:\n",
    "\n",
    "2.1. **Data loading**: Load raw Vectronics data for a given (animal ID, date) pair.\n",
    "\n",
    "2.2. **Windowing**: Partition the data into consecutive 30-second windows.\n",
    "\n",
    "2.3. **Feature computation**: Compute the same nine summary statistics for each window.\n",
    "\n",
    "2.4. **Iteration over metadata**: Repeat steps 1–3 for all unique (animal ID, date) pairs listed in the metadata.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementation Notes\n",
    "All preprocessing steps described above are implemented in  \n",
    "`scripts/run_Vectronics_preprocessing.py`.  \n",
    "This notebook provides a step-by-step explanation of the pipeline and illustrates each stage of the preprocessing process in detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d7f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2382885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.io as io\n",
    "from src.utils.vectronics_preprocessing import (create_max_windows,\n",
    "                                                create_summary_data,\n",
    "                                                load_annotations,\n",
    "                                                create_windowed_features)\n",
    "from src.utils.vectronics_data_prep import (create_vectronics_halfday_segments)\n",
    "import config as config\n",
    "from src.utils.vectronics_data_prep import create_matched_data, create_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2314df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.VECTRONICS_PREPROCESSING_YAML) as f:\n",
    "    Vectronics_preprocessing_config = yaml.safe_load(f)\n",
    "\n",
    "window_duration = Vectronics_preprocessing_config['window_duration']\n",
    "window_length = int(window_duration*config.SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fba1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "\n",
    "    # ---------------- Preprocessing ----------------\n",
    "    min_window_for_padding: Optional[float] = None # minimum duration of signal beyond which pad the signal, None means no padding\n",
    "    seed: int = 0\n",
    "\n",
    "    # ---------------- Data ----------------\n",
    "    source_data_path: Optional[str] = None\n",
    "    \n",
    "args = TrainConfig()\n",
    "\n",
    "np.random.seed(seed=args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1f0f2",
   "metadata": {},
   "source": [
    "## 1. Labeled Vectronics Summary Data\n",
    "\n",
    "We begin by creating half day segments from the raw Vectronics data which is expected to be stored in the following format:  \n",
    "\n",
    "- root data directory\n",
    "  - individual 1\n",
    "    - 2022.csv\n",
    "    - 2023.csv\n",
    "    - 2024.csv\n",
    "  - individual 2\n",
    "    - 2022.csv\n",
    "    - 2023.csv\n",
    "  - individual 3\n",
    "    - 2023.csv\n",
    "    - 2024.csv\n",
    "\n",
    "**Note**: Individual name cannot contain an ``undercsore`` ($\\_$).\n",
    "\n",
    "Each acceleration data CSV is expected to have columns \n",
    "| Column Name                     | Data Type | Description                                      |\n",
    "|---------------------------------|-----------|--------------------------------------------------|\n",
    "| UTC Date[mm/dd]                 | string    | date in format mm/dd                             |\n",
    "| UTC DateTime                    | string    | time in format HH:MM:SS                          |\n",
    "| Milliseconds                    | int       | milliseconds (three digits)                      |\n",
    "| Acc X [g]                       | float     | acceleration reading along X axis                |\n",
    "| Acc Y [g]                       | float     | acceleration reading along Y axis                |\n",
    "| Acc Z [g]                       | float     | acceleration reading along Z axis                |\n",
    "\n",
    "\n",
    "Based on this a `combined_acc` directory is created inside each individual's directory which stores the data split into half days. To easily index the halfday segements, a metadata CSV is created to organize and iterate over the Vectronics files saved as half days. The expected columns in the metadata file are:\n",
    "\n",
    "- `file path`  \n",
    "- `individual ID`  \n",
    "- `year`  \n",
    "- `UTC Date [yyyy-mm-dd]`  \n",
    "- `am/pm`  \n",
    "- `half day [yyyy-mm-dd_am/pm]`  \n",
    "\n",
    "\n",
    "Next, we load the behavior annotations using the function `load_annotations`. This function merges video-based annotations obtained from `io.get_video_labels_path()` with audio-based annotations obtained from `io.get_audio_labels_path()` into a single consolidated table.\n",
    "The resulting `all_annotations` CSV is expected to contain the following columns:\n",
    "\n",
    "- `id`  \n",
    "- `Behavior`  \n",
    "- `Timestamp_start`  \n",
    "- `Timestamp_end`  \n",
    "- `Source`  \n",
    "- `Confidence (H-M-L)`  \n",
    "- `Eating intensity`  \n",
    "- `duration`\n",
    "\n",
    "\n",
    "\n",
    "This metadata enables identification of unique animal–date combinations and provides contextual information (e.g., recording period and temperature) for each Vectronics file.\n",
    "\n",
    "> **Note:** Generating this metadata is computationally expensive and takes approximately **50 minutes** to run. This step should be executed only once to create the metadata corresponding to your local data paths. The resulting file is saved in the `data/` directory and can be reused in subsequent runs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c8c2d",
   "metadata": {},
   "source": [
    "#### Step 1.1: Creating half-day segments and metadata\n",
    "\n",
    "Generating half-day segments and associated metadata from approximately two years of data across five dogs takes **~2.5 hours** to complete. Creating the metadata takes **~20 minutes**. For full-scale processing, it is recommended to run the script `src/utils/vectronics_data_prep.py` in the background (e.g., using `tmux`).\n",
    "\n",
    "In this tutorial, we generate halfdays from **only one chunk (17.36 hours long)** per yearly CSV per individual to validate that the pipeline is functioning correctly. This is achieved by setting `max_chunks=1` in the relevant data-processing functions. \n",
    "\n",
    "Using the default value `max_chunks=None` processes **all chunks**, thereby generating half-day files for the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing individual:         jessie\n",
      "Files for this individual :    ['2023.csv', '2022.csv']\n",
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2022_44934_Samurai_Jessie/2023.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2023-01-01_AM' '2023-01-01_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:39<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2022_44934_Samurai_Jessie/2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2022-05-09_PM' '2022-05-09_AM' '2022-05-10_AM' '2022-05-10_PM'\n",
      " '2022-05-11_AM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:20<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing individual:         green\n",
      "Files for this individual :    ['2021.csv', '2023.csv', '2022.csv']\n",
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44915_Samurai_Green/2021.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2021-03-24_PM' '2021-03-24_AM' '2021-03-25_AM' '2021-03-25_PM'\n",
      " '2021-03-26_AM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:20<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44915_Samurai_Green/2023.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2023-01-26_AM' '2023-02-09_PM'], chunk duration: 0.26 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:00<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44915_Samurai_Green/2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2022-01-01_AM' '2022-01-01_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:15<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing individual:         palus\n",
      "Files for this individual :    ['2021.csv', '2022.csv']\n",
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44910_Aqua_Palus/2021.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2021-03-24_PM' '2021-03-24_AM' '2021-03-25_AM' '2021-03-25_PM'\n",
      " '2021-03-26_AM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:16<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44910_Aqua_Palus/2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2022-01-01_AM' '2022-01-01_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:15<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing individual:         ash\n",
      "Files for this individual :    ['2021.csv', '2022.csv']\n",
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44904_Ninja_Ash/2021.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2021-03-24_PM' '2021-03-24_AM' '2021-03-25_AM' '2021-03-25_PM'\n",
      " '2021-03-26_AM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2021:   0%|          | 0/1 [00:15<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2021_44904_Ninja_Ash/2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2022-01-01_AM' '2022-01-01_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:16<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing individual:         fossey\n",
      "Files for this individual :    ['2023.csv', '2022.csv']\n",
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2022_44907_Aqua_Fossey/2023.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2023-01-01_AM' '2023-01-01_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2023:   0%|          | 0/1 [00:16<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the csv:              /mnt/ssd/medhaaga/wildlife/Vectronics/2022_44907_Aqua_Fossey/2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:00<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half days in chunk:            ['2022-06-12_AM' '2022-06-12_PM' '2022-06-13_AM' '2022-06-13_PM'\n",
      " '2022-06-14_AM' '2022-06-14_PM' '2022-06-23_AM' '2022-07-03_AM'\n",
      " '2022-07-03_PM'], chunk duration: 17.36 hrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022:   0%|          | 0/1 [00:16<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create half-day segments of vectronics data by reading it in chunks\n",
    "create_vectronics_halfday_segments(config.VECTRONICS_PATHS, max_chunks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649d617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual jessie has 507 halfdays.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:00<00:00, 1862.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual green has 900 halfdays.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:00<00:00, 1924.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual palus has 746 halfdays.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 746/746 [00:00<00:00, 1961.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual ash has 799 halfdays.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 799/799 [00:00<00:00, 1832.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual fossey has 888 halfdays.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 888/888 [00:00<00:00, 1543.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# create a metadata of the vectronics data\n",
    "if 0: #os.path.exists(io.get_vectronics_metadata_path()):\n",
    "    print(f\"Metadata is already saved in {io.get_vectronics_metadata_path()}. Creation skipped.\")\n",
    "else:\n",
    "    create_metadata(config.VECTRONICS_PATHS, io.get_vectronics_metadata_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed693544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations and metadata\n",
    "\n",
    "all_annotations = load_annotations()\n",
    "metadata = pd.read_csv(io.get_vectronics_metadata_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86784c58",
   "metadata": {},
   "source": [
    "#### Step 1.2: Annotations Matching\n",
    "\n",
    "To ensure that each 30-second window is associated with a well-defined behavioral label, we enforce a majority-behavior criterion within each window. Specifically, the behavior assigned to a window must be the dominant annotated behavior over that interval. We apply the following procedure:\n",
    "\n",
    "- For each contiguous annotation segment with duration greater than 15 seconds, we extract the corresponding high-frequency Vectronics signal.\n",
    "- These matched signal segments are symmetrically padded on the left and right until a total duration of 30 seconds is reached. \n",
    "- Annotation segments with duration shorter than 15 seconds are discarded later and are not used to create labeled Vectronics summary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2657a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual jessie has 507 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for jessie: 100%|██████████| 507/507 [00:54<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual green has 900 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for green: 100%|██████████| 900/900 [00:35<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual palus has 746 halfdays in the filtered metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing unique half days for palus:   4%|▍         | 31/746 [00:00<00:16, 42.76it/s]\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 9 fields in line 691190, saw 12\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatched raw vectrinics-annotations data already exists in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mio\u001b[38;5;241m.\u001b[39mget_vectronics_data_path()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Creation skipped.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     acc_summary, acc_data, acc_data_metadata, annotations_summary \u001b[38;5;241m=\u001b[39m create_matched_data(filtered_metadata\u001b[38;5;241m=\u001b[39mmetadata, \n\u001b[1;32m      5\u001b[0m                                                             annotations\u001b[38;5;241m=\u001b[39mall_annotations, \n\u001b[1;32m      6\u001b[0m                                                             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      7\u001b[0m                                                             min_window_for_padding\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmin_window_for_padding,\n\u001b[1;32m      8\u001b[0m                                                             min_matched_duration\u001b[38;5;241m=\u001b[39mwindow_duration)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# save matched data, metadata, summaries                                                                              \u001b[39;00m\n\u001b[1;32m     11\u001b[0m     acc_summary\u001b[38;5;241m.\u001b[39mto_csv(io\u001b[38;5;241m.\u001b[39mget_vectronics_summary_path(), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/BotswanaML/notebooks/../src/utils/vectronics_data_prep.py:432\u001b[0m, in \u001b[0;36mcreate_matched_data\u001b[0;34m(filtered_metadata, annotations, verbose, min_window_for_padding, min_matched_duration)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# if the acceleration file is available for this individual and half day, read it\u001b[39;00m\n\u001b[1;32m    431\u001b[0m acc_file_path \u001b[38;5;241m=\u001b[39m individual_metadata\u001b[38;5;241m.\u001b[39mloc[individual_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalf day [yyyy-mm-dd_am/pm]\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m unique_period_loop, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 432\u001b[0m acc_loop \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(acc_file_path)\n\u001b[1;32m    433\u001b[0m acc_loop[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(acc_loop[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m'\u001b[39m, utc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_idx, row \u001b[38;5;129;01min\u001b[39;00m annotations_loop\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m~/miniconda3/envs/wildlife/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/wildlife/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/miniconda3/envs/wildlife/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/wildlife/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 9 fields in line 691190, saw 12\n"
     ]
    }
   ],
   "source": [
    "if 0: # os.path.exists(io.get_vectronics_data_path()):\n",
    "    print(f\"Matched raw vectrinics-annotations data already exists in {io.get_vectronics_data_path()}. Creation skipped.\")\n",
    "else:\n",
    "    acc_summary, acc_data, acc_data_metadata, annotations_summary = create_matched_data(filtered_metadata=metadata, \n",
    "                                                            annotations=all_annotations, \n",
    "                                                            verbose=True, \n",
    "                                                            min_window_for_padding=args.min_window_for_padding,\n",
    "                                                            min_matched_duration=window_duration)\n",
    "\n",
    "    # save matched data, metadata, summaries                                                                              \n",
    "    acc_summary.to_csv(io.get_vectronics_summary_path(), index=False)\n",
    "    acc_data.to_csv(io.get_vectronics_data_path(), index=False)\n",
    "    acc_data_metadata.to_csv(io.get_vectronics_acc_metadata_path(), index=False)\n",
    "    annotations_summary.to_csv(io.get_vectronics_annotations_summary_path(), index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aff1f6",
   "metadata": {},
   "source": [
    "#### Step 1.3: Window Extraction\n",
    "\n",
    "From the matched and padded signal segments obtained in Step 1.1, we extract fixed-length 30-second windows for downstream processing.\n",
    "\n",
    "- For all matched signal chunks with total duration greater than 30 seconds, we segment the data into 30-second windows.\n",
    "- After window extraction, any remaining matched chunks with duration shorter than 30 seconds are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa10fd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating windows of durations 30.0...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating windows of durations {window_duration}...\")\n",
    "acc_data_split = create_max_windows(acc_data=acc_data, window_duration=window_duration, sampling_rate=config.SAMPLING_RATE)\n",
    "acc_data_split = acc_data_split[acc_data_split.duration >= window_duration]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c1537",
   "metadata": {},
   "source": [
    "#### Step 1.4: Feature Computation\n",
    "\n",
    "With the 30-second windows defined in Step 1.2, we compute a set of summary statistics for each window. \n",
    "\n",
    "- Each 30-second window is summarized using **nine predefined statistics** (mean peak-to-peak, max peak-to-peak, amean acceleration for X, Y, and Z axis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e03d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating summary statistics...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating summary statistics...\")\n",
    "df_preprocessed = create_summary_data(acc_data_split, sampling_rate=config.SAMPLING_RATE)\n",
    "df_preprocessed = df_preprocessed.drop(columns=['duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37cb6f",
   "metadata": {},
   "source": [
    "The preprocessed Vectronics data is saved in `args.source_data_path` if it is not `None`. Otherwise, the Vectronics data is stored in the path returned by\n",
    "`io.get_Vectronics_preprocessed_path(args.source_padding_duration)`. When `args.source_padding_duration` is `None` (i.e., no padding is applied), this function resolves to the default path:\n",
    "\n",
    "```\n",
    "/data/Vectronics_preprocessed.csv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed data to /home/medhaaga/BotswanaML/data/Vectronics_preprocessed_duration30.0.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving preprocessed data to {io.get_Vectronics_preprocessed_path(window_duration)}\")\n",
    "if args.source_data_path is not None:\n",
    "    df_preprocessed.to_csv(args.source_data_path, index=False)\n",
    "else:\n",
    "    df_preprocessed.to_csv(io.get_Vectronics_preprocessed_path(window_duration), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007297b",
   "metadata": {},
   "source": [
    "## 2. Unlabeled Vectronics Summary Data\n",
    "\n",
    "The unlabeled Vectronics data are processed similarly to the labeled data but without associating behavioral annotations. Due to the large volume of raw Vectronics recordings, generating the full summary dataset can take approximately **50 minutes** to complete. \n",
    "\n",
    "**Recommendation:**  \n",
    "For full-scale preprocessing, it is advisable to run the script  \n",
    "`scripts/run_Vectronics_preprocessing.py` from the terminal.\n",
    "\n",
    "In this notebook, we demonstrate the preprocessing on a **subset of 10 random (animal ID, date) pairs** as a sanity check. This allows verification of the pipeline without incurring the full runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6683d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = metadata.groupby([\"individual ID\", \"UTC Date [yyyy-mm-dd]\"])\n",
    "group_keys = list(grouped.groups.keys())   # list of (individual, date) tuples\n",
    "np.random.shuffle(group_keys)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fe4fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1708 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1708 [00:16<52:43,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "i = 0\n",
    "\n",
    "for individual, date in tqdm(group_keys, total=len(group_keys)):\n",
    "    group = grouped.get_group((individual, date))\n",
    "\n",
    "    # load all half-day files for this animal/day\n",
    "    dfs = []\n",
    "    for _, row in group.iterrows():\n",
    "        df_half = pd.read_csv(row['file path'])\n",
    "        df_half['Timestamp'] = pd.to_datetime(df_half['Timestamp'], utc=True, format='%Y-%m-%d %H:%M:%S.%f')\n",
    "        dfs.append(df_half)\n",
    "\n",
    "    full_day_data = pd.concat(dfs, ignore_index=True).sort_values(\"Timestamp\")\n",
    "\n",
    "    if len(full_day_data) < window_length:\n",
    "        warnings.warn(f'{individual}-{date} has fewer samples than the window length. Skipped.')\n",
    "        continue\n",
    "\n",
    "    features = create_windowed_features(full_day_data, sampling_frequency=config.SAMPLING_RATE, \n",
    "                                        window_duration=window_duration, window_length=window_length)\n",
    "    features['animal_id'] = individual\n",
    "    features['UTC date [yyyy-mm-dd]'] = date\n",
    "    results.append(features) \n",
    "\n",
    "    i+=1\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "df = pd.concat(results, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving preprocessed data to {io.get_Vectronics_full_summary_path()}\")\n",
    "df.to_csv(io.get_Vectronics_full_summary_path(), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildlife",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
